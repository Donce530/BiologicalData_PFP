# BiologicalData_PFP

Protein function prediction project for Biological Data course in UNIPD. Through this project, we explore the use of various deep learning models, such as CNN, FFNN, MLP and ResNet. Below are the instructions on getting how to train and use the models.

## Setting up the environment

After cloning, start in the root directory of the repository

**Create conda environment:**

```python
conda create --name bio_data_project pip python=3.11
conda activate bio_data_project
```

**Update to the latest pip package manager and use it to install requirements.txt**

```python
pip install --upgrade pip
pip install -r requirements.txt
```

**To confirm that the installation went well run the following command to see if tensorflow can see your GPU:**

```python
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
```

If the command returns a list with at least one GPU device, you are good to go.

After installation, the next step is to correctly configure the project. Open the `config.yml` file and update underlying parameters to fit your needs:

- `directories.raw_data_base` - directory where the unarchived project data is stored (folder containing baseline, test and train subfolders)
- `directories.preprocessed_data.train` - if you want to train the models, this is the directory where the preprocessed training data will be stored
- `directories.preprocessed_data.test` - if you only want to test the models, this is the directory where preprocessed test data will be stored
- `directories.models` - the directory where either your trained models are stored, or the newly trained models get saved to.
- `directories.best_performing_k_fold_models` - temporary directory used during training. After K-fold cross-validation models are evaluated and best performing models for each ontology are stored here. Necessarry for final model training.
- `directories.final_models` - models with the naming 'BP_model', 'CC_model' and 'MF_model' should be stored in this directory. These are the models used to generate the final predictions.
- `directories.prediction_output` - directory where the final prediction file generated by BD_Final_Project.ipynb is stored.
- `use_75_percent_datasets` - flag to use smaller datasets to increase training speed.
- `num_folds` - amount of folds to use in k-fold validation while training
- `batch_size` - batch size for model training, adjust it based on your system resources.

Please note that all the directories provided in the configuration file are expected to exist and *won't* be created by the script.

To run any of the notebooks, you can start a jupyter server by running the command `jupyter lab` from within the conda environment.

## Preprocessing training and evaluation data

In order to to use the models defined within this project, the training data has to be preprocessed. We are working with tensorflow models which accept the data in the format of pandas dataframes. We create 2 dataframes per ontology (Biological Processes, Cellular Components and Molecular Functions) - one for T5 embeddings and one for GO term labels.

To create these datasets, we have prepared a script `preprocess_data.py`. All you need to do is run the script like so: `python preprocess_train_data.py`

## Running model predictions

The repository includes the best performing models for each ontology. 

## Training the models

The model training consisted of two parts - short training with 10-fold validation and then longer training of some specific architectures that look to be performing the best based on the k-fold tests. The k-fold training is done through the notebooks:
- BioDataProject_CNN.ipynb
- BioDataProject_FFNN.ipynb
- BioDataProject_MLP.ipynb
- BioDataProject_ResNet1D.ipynb

for the corresponding models. In each notebook, data from the preprocessed pickle files is loaded and corresponding model architectures are tested.

To reproduce the results:

Finally, the best model is loaded into `BD_Final_Project.ipynb` which performs predictions on the test set and outputs the `Predictions.tsv` file for the CAFA evaluator. This notebook also contains the code and computations for graphs and statistics that are used in the final report.
